import streamlit as st
import openai
import chromadb
import os
# A importa√ß√£o de Counter n√£o √© mais necess√°ria nesta abordagem, mas pode deixar
from collections import Counter 

# --- 1. Configura√ß√£o da P√°gina ---
st.set_page_config(page_title="Evo Assist", page_icon="ü§ñ", layout="wide")

# --- 2. Inje√ß√£o de CSS (Mantido igual) ---
st.markdown("""
<style>
    header {visibility: hidden; height: 0px !important;}
    footer {display: none !important;}
    [data-testid="stHeader"] {display: none !important;}
    [data-testid="stFooter"] {display: none !important;}
    div[class*="container_1upux"] {display: none !important;}
    div[class*="viewerBadge"] {display: none !important;}
    button[title="View fullscreen"] {display: none !important;}
    .block-container {
        padding-top: 0rem !important;
        padding-bottom: 0rem !important;
        padding-left: 1rem !important;
        padding-right: 1rem !important;
        max-width: 100% !important;
    }
    html, body, [data-testid="stAppViewContainer"] {
        font-size: 14px;
        background-color: transparent !important;
    }
    [data-testid="stChatMessage"] {
        padding: 0.5rem !important;
        margin-bottom: 0.5rem !important;
    }
    [data-testid="stChatMessageContent"] p {
        font-size: 0.95rem !important;
        line-height: 1.4 !important;
        overflow-wrap: break-word;
    }
    [data-testid="stVerticalBlock"] > div:first-child {
        margin-top: 0px !important;
        padding-top: 0px !important;
    }
</style>
""", unsafe_allow_html=True)

# --- 3. Configura√ß√£o das Chaves de API (Mantido igual) ---
try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    CHROMA_API_KEY = st.secrets["CHROMA_API_KEY"]
    CHROMA_TENANT = st.secrets["CHROMA_TENANT"]
    CHROMA_DATABASE = st.secrets["CHROMA_DATABASE"]
except (FileNotFoundError, KeyError):
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    CHROMA_API_KEY = os.environ.get("CHROMA_API_KEY")
    CHROMA_TENANT = os.environ.get("CHROMA_TENANT")
    CHROMA_DATABASE = os.environ.get("CHROMA_DATABASE")

if not OPENAI_API_KEY:
    st.error("ERRO: Chave de API da OpenAI n√£o configurada.")
    st.stop()
if not CHROMA_API_KEY or not CHROMA_TENANT or not CHROMA_DATABASE:
    st.error("ERRO: Chaves de API do ChromaDB n√£o configuradas.")
    st.stop()

client_openai = openai.OpenAI(api_key=OPENAI_API_KEY)

# --- 4. Fun√ß√µes do Agente de IA ---
@st.cache_resource
def carregar_colecoes_chroma():
    try:
        _client = chromadb.CloudClient(
            api_key=CHROMA_API_KEY, 
            tenant=CHROMA_TENANT, 
            database=CHROMA_DATABASE
        )
        colecoes_existentes = [col.name for col in _client.list_collections()]
        
        colecao_funcionalidades = None
        if "colecao_funcionalidades" in colecoes_existentes:
            colecao_funcionalidades = _client.get_collection("colecao_funcionalidades")
        else:
            st.warning("Aviso: Cole√ß√£o 'colecao_funcionalidades' n√£o encontrada no banco de dados.")

        colecao_parametros = None
        if "colecao_parametros" in colecoes_existentes:
            colecao_parametros = _client.get_collection("colecao_parametros")
        else:
            st.warning("Aviso: Cole√ß√£o 'colecao_parametros' n√£o encontrada no banco de dados.")
            
        return colecao_funcionalidades, colecao_parametros
    except Exception as e:
        st.error(f"Erro ao conectar com a base de dados Chroma: {e}")
        return None, None

def rotear_pergunta(pergunta):
    prompt_roteador = f"""Classifique a pergunta do usu√°rio. Responda APENAS: FUNCIONALIDADE, PARAMETRO ou SAUDACAO. Pergunta: '{pergunta}'"""
    try:
        resposta = client_openai.chat.completions.create(
            model="gpt-4o", messages=[{"role": "user", "content": prompt_roteador}], temperature=0, max_tokens=10
        )
        intencao = resposta.choices[0].message.content.strip().upper()
        if "FUNCIONALIDADE" in intencao: return "FUNCIONALIDADE"
        if "PARAMETRO" in intencao: return "PARAMETRO"
        return "SAUDACAO"
    except Exception as e:
        return "SAUDACAO"

# --- FUN√á√ÉO DE BUSCA REFORMULADA (A SOLU√á√ÉO) ---
def buscar_e_sintetizar_contexto(pergunta, colecao, n_results_inicial=10):
    if colecao is None:
        st.warning("Tentativa de busca em uma cole√ß√£o inexistente.")
        return "", None
    try:
        # 1. Gera o embedding da pergunta
        emb_response = client_openai.embeddings.create(input=[pergunta], model="text-embedding-3-small")
        emb = emb_response.data[0].embedding
        
        # --- NOVO: Pr√©-filtragem baseada em Palavras-Chave Cr√≠ticas ---
        pergunta_lower = pergunta.lower()
        filtro_hard = None # Padr√£o: sem filtro

        # Mapa de palavras-chave para substrings dos nomes das features no JSON
        # Se o usu√°rio disser a chave, for√ßamos o filtro pelo valor.
        keyword_map = {
            "pedido": "Pedido",        # Vai casar com "Acompanhamento de Pedidos..."
            "solicita√ß√£o": "Solicita√ß√£o", # Vai casar com "Solicita√ß√£o de Compra..."
            "solicitacao": "Solicita√ß√£o"  # Varia√ß√£o sem acento
            # Adicione outros pares cr√≠ticos aqui se necess√°rio
        }

        conditions = []
        for keyword, feature_substring in keyword_map.items():
            # Se a palavra-chave cr√≠tica est√° na pergunta...
            if keyword in pergunta_lower:
                # ...adiciona uma condi√ß√£o de filtro para o banco de dados.
                # Usamos "$contains" para buscar a substring no nome completo da feature.
                conditions.append({"feature_name": {"$contains": feature_substring}})
        
        # Monta o filtro final para o ChromaDB
        if conditions:
            # Usamos um 'set' de tuplas para remover duplicatas (ex: se achar "solicita√ß√£o" e "solicitacao")
            unique_conditions = [dict(t) for t in {tuple(d.items()) for d in conditions}]
            
            if len(unique_conditions) == 1:
                filtro_hard = unique_conditions[0]
            elif len(unique_conditions) > 1:
                # Se o usu√°rio mencionou ambos (raro, mas poss√≠vel), permite ambos.
                filtro_hard = {"$or": unique_conditions}
            
            # print(f"DEBUG: Aplicando filtro HARD por palavra-chave: {filtro_hard}")
        # -------------------------------------------------------------

        # 2. Realiza a busca sem√¢ntica, APLICANDO O FILTRO SE HOUVER
        res = colecao.query(
            query_embeddings=[emb], 
            n_results=n_results_inicial,
            where=filtro_hard # <-- Aqui est√° a m√°gica. Se tiver filtro, ele usa.
        )
        meta = res.get('metadatas', [[]])[0]
        
        if not meta:
            return "", None

        # 3. Sele√ß√£o de V√≠deo Simplificada e Confi√°vel
        # Como j√° filtramos os resultados para conter APENAS a funcionalidade correta
        # (se a palavra-chave foi usada), podemos pegar o primeiro v√≠deo que aparecer com seguran√ßa.
        video = None
        for m in meta:
            v_url = m.get('video_url')
            if v_url:
                video = v_url
                break
        
        # Monta o contexto
        contexto = "\n\n---\n\n".join([doc.get('texto_original', '') for doc in meta if doc.get('texto_original')])
        
        return contexto, video
    except Exception as e:
        st.error(f"Erro durante busca e s√≠ntese de contexto: {e}")
        return "", None
# --- FIM DA FUN√á√ÉO REFORMULADA ---

def gerar_resposta_sintetizada(pergunta, contexto, prompt_sistema):
    prompt_usuario = f"""Use o seguinte contexto para responder √† pergunta do usu√°rio.
Se a resposta n√£o puder ser encontrada no contexto, diga que voc√™ n√£o tem essa informa√ß√£o.
Seja claro, conciso e direto.

CONTEXTO:
{contexto}

PERGUNTA:
{pergunta}

RESPOSTA:"""
    try:
        resposta = client_openai.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": prompt_sistema},
                {"role": "user", "content": prompt_usuario}
            ],
            temperature=0.3
        )
        return resposta.choices[0].message.content
    except Exception as e:
        return "Desculpe, ocorreu um erro ao gerar a resposta."

# --- 5. L√≥gica do Chat (Mantido igual) ---
P_FUNC_SYSTEM = """Voc√™ √© o Evo, um assistente virtual especializado no sistema GoEvo.
Sua fun√ß√£o √© ajudar usu√°rios com d√∫vidas sobre funcionalidades e processos do sistema.
- Suas respostas devem ser baseadas **exclusivamente** no contexto fornecido.
- Seja direto, claro e objetivo.
- Use listas numeradas ou bullet points para instru√ß√µes passo a passo.
- Se o contexto n√£o contiver a informa√ß√£o, admita que n√£o sabe. N√£o invente."""

P_PARAM_SYSTEM = """Voc√™ √© o Evo, um especialista t√©cnico do sistema GoEvo.
Sua fun√ß√£o √© explicar o significado e o prop√≥sito de par√¢metros, campos e configura√ß√µes espec√≠ficas do sistema.
- Suas explica√ß√µes devem ser curtas, precisas e f√°ceis de entender.
- Baseie-se **exclusivamente** no contexto t√©cnico fornecido.
- Se o contexto n√£o tiver a defini√ß√£o, diga que n√£o encontrou a informa√ß√£o."""

RES_SAUDACAO = "Ol√°! Eu sou o Evo, seu assistente virtual para o sistema GoEvo. Como posso ser √∫til hoje?"

colecao_func, colecao_param = carregar_colecoes_chroma()

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": RES_SAUDACAO}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and "video" in msg and msg["video"]:
            st.video(msg["video"])

if pergunta := st.chat_input("Qual a sua d√∫vida sobre o GoEvo?"):
    st.session_state.messages.append({"role": "user", "content": pergunta})
    with st.chat_message("user"):
        st.markdown(pergunta)

    with st.chat_message("assistant"):
        with st.spinner("Processando sua pergunta..."):
            intencao = rotear_pergunta(pergunta)
            video_mostrar = None
            res_final = ""
            
            if intencao == "SAUDACAO":
                res_final = RES_SAUDACAO
            else:
                col = colecao_func if intencao == "FUNCIONALIDADE" else colecao_param
                prompt_sis = P_FUNC_SYSTEM if intencao == "FUNCIONALIDADE" else P_PARAM_SYSTEM
                
                if col:
                    ctx, video_mostrar = buscar_e_sintetizar_contexto(pergunta, col)
                    if ctx:
                        res_final = gerar_resposta_sintetizada(pergunta, ctx, prompt_sis)
                    else:
                        res_final = "Desculpe, n√£o encontrei informa√ß√µes relevantes sobre isso na minha base de conhecimento."
                else:
                     res_final = "Desculpe, a base de conhecimento necess√°ria n√£o est√° dispon√≠vel no momento."

            st.markdown(res_final)
            if video_mostrar:
                st.video(video_mostrar)
    
    st.session_state.messages.append({"role": "assistant", "content": res_final, "video": video_mostrar})
